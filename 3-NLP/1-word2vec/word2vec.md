## **高效的词表示向量空间估计**

**摘要**
- **目标**: 提出两种新颖的模型架构，用于从大型数据集中计算词的连续向量表示。
- **结果评估**: 在词相似性任务中测量这些表示的质量，并将结果与基于不同类型神经网络的先前最佳技术进行比较。
- **观察**: 观察到在更低的计算成本下大幅提高准确性，即从16亿词的数据集中学习高质量词向量需要的时间不到一天。
- **进一步展示**: 这些向量在测试集上提供了测量句法和语义词相似性的最先进性能。

### **1 引言**
- **背景**: 许多当前的自然语言处理(NLP)系统和技术将词视为原子单位——词之间没有相似性的概念，这些词被表示为词汇表中的索引。这种选择有几个好处——简单性、稳健性以及在大量数据上训练的简单模型优于在较少数据上训练的复杂系统的观察。
- **局限性**: 然而，这些简单技术在许多任务中已达到其极限。例如，自动语音识别中相关的领域数据是有限的，通常性能由高质量转录语音数据的大小（通常仅为数百万词）所主导。在机器翻译中，许多语言的现有语料库只包含几十亿词或更少。因此，在某些情况下，简单地扩大基本技术的规模不会带来任何显著进展，我们必须专注于更高级的技术。
- **分布式表示的成功**: 随着近年来机器学习技术的进步，训练更复杂的模型成为可能，这些模型通常优于简单模型。可能最成功的概念是使用词的分布式表示。例如，基于神经网络的语言模型显著优于N-gram模型。

### **1.1 论文目标**
- **主要目标**: 介绍可以用于从包含数十亿词和数百万词词汇量的巨大数据集中学习高质量词向量的技术。据我们所知，没有任何先前提出的架构成功地在如此大的数据集上进行过训练。

---
接下来，我将继续提取并总结下一页内容。

## **词向量表示的高效估计**

### **1.2 先前工作**
- **词向量的历史**: 将词表示为连续向量有着悠久的历史。一种非常流行的用于估计神经网络语言模型(NNLM)的模型架构在文献中被提出，其中使用带有线性投影层和非线性隐藏层的前馈神经网络来共同学习词向量表示和统计语言模型。
- **其他NNLM架构**: 另一种有趣的NNLM架构在使用单隐藏层的神经网络首先学习词向量。然后使用这些词向量来训练NNLM。因此，即使没有构建完整的NNLM，也可以学习词向量。本文直接扩展了这一架构，仅关注使用简单模型学习词向量的第一步。

### **2 模型架构**
- **不同模型类型**: 提出了许多不同类型的模型来估计词的连续表示，包括著名的潜在语义分析(LSA)和潜在狄利克雷分配(LDA)。
- **本文重点**: 本文聚焦于由神经网络学习的词的分布式表示，因为先前已显示它们在保持词之间的线性规律性方面比LSA表现得更好；LDA在大数据集上计算上非常昂贵。
- **模型架构比较**: 类似于之前的研究，通过定义模型的计算复杂度（即训练模型需要访问的参数数量）来比较不同模型架构。接下来，将尝试在最小化计算复杂度的同时最大化准确度。

---
这部分强调了研究词向量的历史背景和不同模型架构的比较。下一步，我将提取并总结第三页的内容。

## **模型架构**

### **2.1 前馈神经网络语言模型 (NNLM)**
- **基础**: NNLM由输入层、投影层、隐藏层和输出层组成。在输入层，使用1-of-V编码对N个前词进行编码，V是词汇表的大小。
- **投影层**: 输入层通过共享投影矩阵映射到具有N*D维度的投影层P，由于任何给定时间只有N个输入激活，因此投影层的组合是一个相对廉价的操作。
- **计算复杂度**: NNLM架构在投影层与隐藏层之间的计算变得复杂，因为投影层中的值是密集的。计算复杂度每个训练样例为 $Q = N*D + N*D*H + H*V$，其中$H*V$项占据主导地位。
- **优化**: 为了避免$H*V$的高计算成本，提出了几种实际解决方案，包括使用softmax的层次化版本或使用训练期间未归一化的模型。通过词汇表的二叉树表示，需要评估的输出单元数量可以降低至大约$log_2(V)$。使用霍夫曼二叉树进一步减少了需要评估的输出单元数量，因为它为频繁词分配了短二进制码。

### **2.2 循环神经网络语言模型 (RNNLM)**
- **创新**: RNNLM旨在克服前馈NNLM的某些限制，例如需要指定上下文长度（模型的阶数 $N$），并且理论上RNN可以比浅层神经网络更有效地表示更复杂的模式。
- **结构**: RNN模型没有投影层；只有输入层、隐藏层和输出层。此类型模型的特殊之处在于连接隐藏层和其自身的循环矩阵，使用时延连接。这允许循环模型形成某种短期记忆，因为过去的信息可以通过基于当前输入和上一时间步隐藏层状态更新的隐藏层状态来表示。
- **计算复杂度**: RNN模型每个训练样例的复杂度为 $Q = H*H + H*V$，其中词表示 $D$ 与隐藏层 $H$ 的维度相同。同样，$H*V$ 项可以通过使用层次化softmax有效减少到 $H*log_2(V)$。因此，大部分复杂度来自于 $H*H$。

这部分介绍了两种关键的神经网络语言模型架构——前馈NNLM和循环RNNLM，着重于它们的结构、计算复杂度以及如何通过使用层次化softmax等方法来优化这些复杂度。接下来，我将继续提取并总结第四页的内容。

## **新的对数线性模型**

### **3 新型对数线性模型**
- **背景**: 本节提出两种新的模型架构，用于学习词的分布式表示，旨在最小化计算复杂度。先前章节的主要观察是大部分复杂度由模型中的非线性隐藏层引起。虽然这是神经网络吸引人的特点，但本研究旨在探索可能不如神经网络精确但能更高效地在大量数据上训练的简化模型。

### **3.1 连续词袋模型 (CBOW)**
- **架构特点**: 类似于前馈NNLM，但移除了非线性隐藏层，且投影层对所有词共享（不仅是投影矩阵）；因此，所有词被投影到相同位置（它们的向量被平均）。
- **模型称谓**: 由于历史中词的顺序不影响投影，故称此架构为词袋模型。此外，本模型还利用了未来的词；通过在输入中构建一个带有四个未来和四个历史词的对数线性分类器，在下一节介绍的任务中获得了最佳性能，训练准则是正确分类当前（中间）词。
- **训练复杂度**: $Q = N * D + D * log_2(V)$。

### **3.2 连续跳跃模型 (Skip-gram)**
- **架构特点**: 类似于CBOW，但不是基于上下文预测当前词，而是尝试最大化基于同一句子中另一个词的词分类。更准确地说，我们使用每个当前词作为对数线性分类器的输入，预测在当前词前后一定范围内的词。
- **效果与复杂度**: 发现增加范围可以提高结果词向量的质量，但也会增加计算复杂度。由于更远的词通常与当前词的相关性较低，我们通过在训练样本中较少地从这些词中抽样，给予远距离词较少的权重。
- **训练复杂度**: $Q = C * (D + D * log_2(V))$，其中$C$是词的最大距离。

这两种新型模型架构——CBOW和Skip-gram，都旨在以较低的计算复杂度学习词的分布式表示。CBOW通过平均上下文词向量来预测中心词，而Skip-gram则通过当前词来预测上下文中的词，两者都有效降低了模型的计算需求，使得可以在更大数据集上进行训练。接下来，我将继续提取并总结第五页的内容。

## **结果与模型架构解析**

### **4 结果**
- **质量比较**: 为了比较不同版本词向量的质量，先前的论文通常使用一个表格展示示例词及其最相似的词，直观理解它们之间的关系。虽然易于展示词如“France”与“Italy”之间的相似性，但在更复杂的相似性任务中测试这些向量则更具挑战性。
- **相似性类型**: 之前的观察指出，词之间可以有许多不同类型的相似性。例如，“big”与“bigger”之间的相似性，与“small”与“smaller”之间的相似性类似。另一类型的关系可以是词对“big - biggest”和“small - smallest”。
- **代数操作**: 令人惊讶的是，通过对词向量进行简单的代数操作，可以回答这样的问题。例如，要找到与“small”在同一感觉上类似于“biggest”与“big”相似的词，可以简单地计算向量 $X = vector("biggest") - vector("big") + vector("small")$。然后，在向量空间中搜索最接近$X$的词（通过余弦距离测量），作为问题的答案（在此搜索过程中排除输入的问题词）。当词向量训练得当时，可以使用这种方法找到正确答案（如“smallest”）。

- **高维词向量的训练与应用**: 发现在大量数据上训练高维词向量时，生成的向量可以用来回答词之间非常微妙的语义关系，例如城市及其所属国家之间的关系（如法国之于巴黎，德国之于柏林）。具有这种语义关系的词向量可以用于改进许多现有的自然语言处理应用，如机器翻译、信息检索和问答系统，并可能启用未来尚未发明的其他应用。

这一部分展示了利用向量代数操作探索词之间复杂的语义关系的能力，强调了在大规模数据集上训练高质量、高维度的词向量的重要性及其在多种自然语言处理应用中的潜在用途。接下来，我将继续提取并总结第六页的内容。

## **语义-句法词关系测试集及其评估**

### **4.1 任务描述**
- **测试集定义**: 定义了一个全面的测试集，包含五种类型的语义问题和九种类型的句法问题，以衡量词向量的质量。测试集中总共有8869个语义问题和10675个句法问题。
- **问题创建**: 每个类别中的问题是通过两步创建的：首先，手动创建一系列相似词对。然后，通过连接两对词对形成大量问题。例如，创建了一个包含68个大型美国城市及其所属州的列表，通过随机选择两对词对形成大约2500个问题。
- **评估准确性**: 评估所有问题类型的总体准确率，以及每个问题类型（语义、句法）的准确率。问题仅在使用上述方法计算的向量的最接近词与问题中的正确词完全相同时才被认为是正确回答的；同义词被计为错误。

### **4.2 准确性最大化**
- **训练数据**: 使用一个包含约6B令牌的Google News语料库训练词向量，限制词汇量为最常用的100万个词。
- **优化问题**: 面对时间受限的优化问题，预期使用更多数据和更高维度的词向量将提高准确性。为了估计获取尽可能好结果的最佳模型架构选择，首先评估了在训练数据子集上训练的模型，其中词汇量限制为最常用的30k词。
- **数据与维度**: 发现在某一点之后，增加更多维度或更多训练数据提供的改进递减。因此，需要同时增加向量维度和训练数据量。尽管这一观察可能看似显而易见，但必须指出，目前流行的做法是在相对大量的数据上训练词向量，但向量的大小不足。

这部分内容详细介绍了语义-句法词关系测试集的构建和评估方法，强调了在大规模语料库上训练高维度词向量的重要性及其对提高准确性的潜在影响。接下来，我将继续提取并总结第七页的内容。

## **模型架构比较与准确性评估**

### **表2: CBOW架构词向量的准确性**
- **数据**: 使用CBOW架构的词向量在限定词汇的语义-句法词关系测试集上的准确性。只使用了最常用的30k词中的问题。
- **结果**: 随着训练数据量和向量维度的增加，准确性提高。例如，在783M训练词下，50维词向量的准确性为23.2%，而600维词向量的准确性为50.4%。

### **表3: 不同架构模型的比较**
- **数据**: 使用相同训练数据，640维词向量，比较不同模型架构导出的词向量。实验涉及全套语义-句法词关系测试集问题。
- **结果**: 
  - **RNNLM**在句法问题上表现良好，语义准确性为9%，句法准确性为36%。
  - **NNLM**比RNN表现更好，语义准确性为23%，句法准确性为53%。
  - **CBOW**在句法任务上优于NNLM，语义准确性约为24%，句法准确性为64%。
  - **Skip-gram**在语义部分的测试中比其他所有模型表现得都要好，语义准确性为55%，句法准确性稍低于CBOW模型但仍优于NNLM。

### **实验设置**
- **训练数据与周期**: 实验在使用随机梯度下降和反向传播的三个训练周期中完成。选择起始学习率为0.025，并在最后一个训练周期结束时线性降至零。
- **数据来源**: 实验数据包括几个LDC语料库，详细描述在先前研究中，包含320M词汇，82K词汇量。

这部分内容比较了不同模型架构在同等条件下产生的词向量的性能，突出了Skip-gram架构在语义任务上的优异表现，同时也展示了随着训练数据和向量维度增加，准确性提高的趋势。接下来，我将继续提取并总结第八页的内容。

## **公开可用词向量与我们模型词向量的比较**

### **表4: 词向量比较**
- **模型对比**: 展示了公开可用词向量与本研究模型词向量在语义-句法词关系测试集上的性能比较。使用了完整词汇表。
- **结果**:
  - 公开词向量的准确性普遍较低，例如Collobert-Weston NNLM的总准确性为11.0%，而Turian NNLM的最低至1.8%。
  - 本研究的NNLM模型在不同维度下表现出显著的性能提升，100维词向量在6B训练词下的总准确性达到50.8%。
  - CBOW和Skip-gram模型表现更佳，尤其是Skip-gram模型在300维、783M训练词下的总准确性达到53.3%。

### **表5: 不同训练周期的模型比较**
- **训练周期对比**: 比较了在相同数据上训练三个周期与一个周期的模型的准确性。
- **结果**:
  - 一个训练周期下的模型提供了与三个训练周期相当或更好的结果，并且提供了额外的小幅速度提升。例如，单周期的Skip-gram模型在1.6B训练词下的总准确性达到53.8%，而三周期的同配置Skip-gram模型的总准确性为53.3%。
- **训练时间**: CBOW模型在大约一天内完成训练，而Skip-gram模型的训练时间约为三天。

### **4.4 大规模并行训练模型**
- **实施框架**: 在称为DistBelief的分布式框架中实现了各种模型，利用mini-batch异步梯度下降和名为Adagrad的自适应学习率程序进行训练。
- **训练配置**: 训练期间使用了50至100个模型副本，这些模型在Google News 6B数据集上进行训练。

这部分内容展示了不同词向量模型在大规模训练数据集上的性能比较，强调了Skip-gram模型在维度、训练词量及训练周期方面的优越性能，以及大规模并行训练框架的应用，使得在大数据集上的训练成为可能。接下来，我将继续提取并总结第九页的内容。

## **大规模并行训练模型比较**

### **表6: 使用DistBelief分布式框架训练的模型比较**
- **注意**: 使用1000维向量训练NNLM将耗时过长，因此未完成。
- **结果**:
  - **NNLM**（100维，6B训练词）的总准确性为50.8%，训练时间为14天x180CPU核。
  - **CBOW**（1000维，6B训练词）的总准确性为63.7%，训练时间为2天x140CPU核。
  - **Skip-gram**（1000维，6B训练词）的总准确性为65.6%，训练时间为2.5天x125CPU核。

### **表7: 微软研究院句子完成挑战的模型比较和组合**
- **挑战描述**: 该挑战最近被引入作为推进语言建模和其他自然语言处理技术的任务。任务包含1040个句子，每个句子缺少一个词，目标是从五个合理选项中选择最符合句子其余部分的词。
- **结果**:
  - **Skip-gram**模型单独在此任务上的表现虽不优于LSA相似度，但与RNNLMs得到的分数互补，加权组合后达到新的最高记录，准确性为58.9%。

### **学习到的关系示例**
- **示例**: 展示了遵循各种关系的词。例如，通过计算 `Paris - France + Italy` 得到 `Rome`。尽管准确性相当好，但仍有进一步改进的空间。

这一部分内容展示了使用DistBelief分布式框架进行大规模并行训练的模型性能，特别是1000维Skip-gram模型在语义-句法词关系测试集上达到的高准确性，以及Skip-gram模型在微软研究院句子完成挑战上的应用，展示了其与RNNLMs的互补性。接下来，我将继续提取并总结第十页的内容。

## **学习到的关系示例及结论**

### **表8: 单词对关系示例**
- **使用**: 表4中最佳词向量（在783M词上训练的300维Skip-gram模型）的示例。
- **示例**: 包括国家和首都（如法国-巴黎、意大利-罗马）、比较级（如大-更大、小-更大）、人物和职业（如爱因斯坦-科学家、梅西-中场）、公司和产品（如微软-Windows、谷歌-Android）等多种关系。

### **6 结论**
- **研究发现**: 通过各种模型在语法和语义任务集合上得出的词向量质量进行了研究。观察到使用非常简单的模型架构就能训练出高质量的词向量，与流行的神经网络模型（前馈和循环）相比。
- **计算复杂度**: 由于计算复杂度大大降低，可以从更大的数据集中计算出非常准确的高维词向量。使用DistBelief分布式框架，甚至可以在拥有一万亿词的语料库上训练CBOW和Skip-gram模型，对词汇量几乎没有限制，这比之前类似模型公布的最佳结果大几个数量级。
- **应用前景**: 神经网络基础的词向量之前已应用于多个自然语言处理任务，如情感分析和释义检测。可以预期，本文描述的模型架构将有助于这些应用。正在进行的工作显示，词向量可以成功应用于知识库的自动扩展及现有事实的验证。机器翻译实验的结果也非常有前景。未来，比较我们的技术与潜在关系分析等其他技术也会很有趣。
- **研究贡献和期望**: 本文提供的综合测试集将帮助研究社区改进现有的词向量估计技术。也预期高质量的词向量将成为未来自然语言处理应用的重要构建块。

本文通过各种实验和比较，展示了简单模型架构在训练高质量词向量方面的有效性，特别是Skip-gram模型在大规模数据上的应用潜力，以及词向量在未来自然语言处理任务中的广泛应用前景。