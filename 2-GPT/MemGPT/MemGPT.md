https://chat.openai.com/share/8d9a8335-dafc-4847-aced-a24891a5ad46

---

### MemGPT: 向着作为操作系统的大型语言模型发展

#### **摘要**
大型语言模型（LLMs）已经革新了人工智能领域，但由于受限的上下文窗口，其在扩展对话和文档分析等任务中的应用受到限制。为了能够使用超出有限上下文窗口的上下文，我们提出了**虚拟上下文管理**技术，该技术受到传统操作系统中分层内存系统的启发，通过在物理内存和磁盘之间分页，提供扩展虚拟内存的假象。利用这种技术，我们引入了MemGPT（MemoryGPT），一个系统，它能智能管理不同的存储层次，以在LLM的有限上下文窗口内有效提供扩展的上下文。我们在两个领域评估了我们的操作系统灵感设计，这两个领域中现代LLM的有限上下文窗口严重限制了它们的性能：文档分析，MemGPT能够分析远超过底层LLM上下文窗口的大型文档；多会话聊天，MemGPT可以创建记住、反映并通过与用户的长期互动动态发展的对话代理。我们在 https://research.memgpt.ai 发布了MemGPT的代码和实验数据。

#### **1. 引言**
近年来，大型语言模型（LLMs）及其底层的变换器架构已成为会话AI的基石，并已引领消费者和企业应用的广泛阵列。尽管有这些进步，LLMs使用的有限固定长度上下文窗口显著限制了它们在长对话或关于长文档推理的应用性。例如，最广泛使用的开源LLM在进行长对话或分析长文档时表现不佳。

#### **2. MemGPT设计**
为了克服这些限制，我们提出了MemGPT，一个旨在通过智能地管理内存层来扩展LLM上下文窗口的系统。MemGPT的设计灵感来源于操作系统中的虚拟内存管理，它通过在不同的存储级别（如RAM和磁盘）之间分页，提供了一个扩展的上下文窗口。这使得LLM能够访问比其原生上下文窗口更大的信息量，从而在长对话和大型文档分析等任务中表现更好。

- **2.1 主上下文（prompt tokens）**：在MemGPT中，prompt tokens被划分为三个连续部分：系统指令、工作上下文和FIFO队列。系统指令是只读的（静态的），包含关于MemGPT控制流程的信息，不同内存级别的预期用途，以及如何使用MemGPT函数的指令。工作上下文是一个固定大小的可读写文本块，只能通过MemGPT函数调用进行写入。FIFO队列存储消息的滚动历史，包括代理和用户之间的消息、系统消息（如内存警告）以及函数调用的输入和输出。
- **2.2 队列管理器**：队列管理器管理回忆存储中的消息和FIFO队列。当系统接收到新消息时，队列管理器将传入的消息追加到FIFO队列，触发LLM推理生成输出（完成的tokens）。队列管理器将传入的消息和生成的LLM输出写入回忆存储（MemGPT消息数据库）。

此概要介绍了MemGPT的基本概念和设计理念，展示了如何通过虚拟上下文管理扩展LLMs的上下文窗口，以及MemGPT在文档分析和多会话聊天领域的应用潜力。接下来，我将继续提取和总结接下来的两页内容。

### **3. MemGPT的实现和应用**

#### **3.1 MemGPT对话代理**
对话代理，如虚拟伴侣和个性化助手，旨在与用户进行自然的、长期的互动，可能持续数周、数月甚至数年。这对于具有固定长度上下文的模型构成挑战，因为它们只能引用对话历史中的有限信息。一个“无限上下文”代理应该能够无缝处理连续的交流，没有边界或重置。

为了满足用户的需求，这样的代理必须满足两个关键标准：**一致性** - 代理应保持对话的连贯性。新的事实、偏好和事件的提及应与用户和代理以前的声明保持一致；**参与度** - 代理应利用对用户的长期了解来个性化交流。

#### **MemGPT的结构和功能执行**
MemGPT通过增加一个分层内存系统和功能，使固定上下文LLM处理器能够管理自己的内存。LLM的提示令牌（输入）或主上下文由系统指令、工作上下文和FIFO队列组成。LLM的完成令牌（输出）被作为函数调用由函数执行器解释。MemGPT使用函数在主上下文和外部上下文（归档和回忆存储数据库）之间移动数据。LLM可以通过在其输出中生成特殊关键字参数（`request_heartbeat=true`）来请求立即的后续LLM推断，以串联函数调用，从而执行多步检索以回答用户查询。

队列管理器还负责通过队列驱逐策略控制上下文溢出。当提示令牌超过LLM上下文窗口的“警告令牌计数”（例如，上下文窗口的70%）时，队列管理器将插入一个系统消息到队列中，警告LLM即将发生的队列驱逐（“内存压力”警告），以允许LLM使用MemGPT功能将FIFO队列中包含的重要信息存储到工作上下文或归档存储（一个读/写数据库，存储任意长度的文本对象）。

#### **实施细节和基准测试**
在讨论OpenAI模型时，除非另有说明，“GPT-4 Turbo”指的是特定的`gpt-4-1106-preview`模型端点（上下文窗口为128,000），“GPT-4”指的是`gpt-4-0613`（上下文窗口为8,192），而“GPT-3.5 Turbo”指的是`gpt-3.5-turbo-1106`（上下文窗口为16,385）。

在实验中，我们使用所有基线模型（GPT-4、GPT-4 Turbo和GPT-3.5）运行MemGPT，以展示底层模型性能如何影响MemGPT的性能。对于对话分析，我们在现有的任务上对MemGPT进行基准测试，这些任务来自（Liu et al., 2023a）针对长文档的问答和键值检索。我们还提出了一个新的嵌套键值检索任务，需要汇总多个数据源的信息，这测试了代理汇总多个数据源信息（多跳检索）的能力。我们公开发布了我们增强的MSC数据集、嵌套KV检索数据集，以及20M维基百科文章的嵌入数据集，以促进未来的研究。

此部分详细介绍了MemGPT的结构、功能执行方式以及如何通过队列管理器和函数调用处理上下文管理。同时，提供了关于实施细节和基准测试的信息，说明了MemGPT如何在对话代理和文档分析等领域应用。接下来，我将继续提取和总结接下来的两页内容。

### **3. MemGPT的性能和应用**

#### **3.1.1 深度记忆检索任务（一致性）**
MemGPT在深度记忆检索（DMR）任务中表现出色，该任务要求代理关于之前对话中讨论的主题提出特定问题。代理的回答与标准答案进行比较，MemGPT显著优于固定上下文基线模型。例如，与GPT-3.5 Turbo相比，MemGPT的准确率提高到66.9%，ROUGE-L得分为0.629；与GPT-4相比，MemGPT的准确率提高到92.5%，ROUGE-L得分为0.814。这显示了MemGPT利用其内存改进对话一致性的能力，能够记住过去互动中的相关事实、偏好和事件，以维持连贯性。

#### **3.2 多文档问答**
MemGPT在多文档问答任务中的能力也进行了测试，该任务通过与固定上下文基线模型对比来评估MemGPT分析文档的能力。在这个任务中，从NaturalQuestions-Open数据集中选择一个问题，检索器选择与问题相关的维基百科文档。然后，将这些文档作为输入提供给读者模型（LLM），并要求使用提供的文档回答问题。评估阅读器准确性时，随着检索的文档数量增加，对比固定上下文基线，MemGPT使用相同的检索器，根据相似度搜索（余弦距离）选择顶部K个文档，使用PostgreSQL进行归档记忆存储，通过pgvector扩展启用向量搜索。预计算嵌入并将其加载到数据库中，使用HNSW索引来实现近似、亚秒级的查询时间。在MemGPT中，整个嵌入文档集加载到归档存储中，检索器通过归档存储搜索功能自然出现（基于余弦相似度执行向量搜索）。与固定上下文基线不同，在原始检索器-阅读器设置中，顶部K个文档是独立于LLM推理使用检索器获取的。

通过这些评估，MemGPT展示了在一致性和参与度方面，相比于固定上下文基线模型，如何更好地处理长期对话交互的挑战。MemGPT的无界上下文提供了对话代理有意义的好处，包括改善对话的自然度和参与感，以及增强的文档分析能力。这证明了MemGPT在提供扩展上下文和处理复杂对话及文档分析任务方面的潜力。接下来，我将继续提取和总结接下来的两页内容。

### **5. 结论**

在这篇论文中，我们介绍了MemGPT，这是一个受操作系统启发的新型大型语言模型（LLM）系统，旨在管理LLM的有限上下文窗口。通过设计类似于传统操作系统的内存层次结构和控制流，MemGPT为LLM提供了更大上下文资源的假象。这种受OS启发的方法在两个领域进行了评估，这两个领域现有LLM的性能受到有限上下文长度的限制：文档分析和对话代理。

- **文档分析**：MemGPT能够有效地处理远远超出当前LLM上下文限制的长文本，通过有效地在内存中分页进出相关上下文。
- **对话代理**：MemGPT使得维持长期记忆、一致性和在扩展对话中的可演化性成为可能。

总体而言，MemGPT展示了操作系统技术，如分层内存管理和中断，即使在受限的固定上下文长度下，也可以释放LLM的潜力。这项工作为未来的探索开辟了众多途径，包括将MemGPT应用于具有巨大或无界上下文的其他领域、整合不同的内存层技术（如数据库或缓存），以及进一步改进控制流和内存管理策略。通过将操作系统架构的概念引入AI系统，MemGPT代表了一种有前途的新方向，用于在其基本限制内最大化LLM的能力。

### **嵌套键值检索任务性能**

MemGPT是唯一能够一致完成超过2个嵌套级别的嵌套键值（KV）任务的方法。虽然GPT-4 Turbo作为基线表现更好，但与GPT-4 Turbo结合使用的MemGPT比与GPT-4结合使用的MemGPT表现更差。

此外，论文还讨论了其他相关工作，如Yao等（2022）展示了交错的思维链推理可以进一步提高基于LLM的交互式代理的计划能力；类似地，在MemGPT中，LLM能够在执行函数时“大声计划”。Liu等（2023b）引入了一套LLM-as-an-agent基准测试，以评估LLM在交互式环境中的表现，包括视频游戏、思维谜题和网络购物。与之形成对比的是，我们的工作侧重于解决装备代理长期记忆用户输入的问题。

这一部分总结了MemGPT的创新点、性能优势以及如何应对有限上下文窗口的挑战，展示了其在文档分析和对话代理领域的应用潜力。MemGPT将操作系统的概念引入AI系统，为扩展LLM的能力提供了新的可能性。接下来，我将继续提取和总结接下来的两页内容。