https://chat.openai.com/share/f8d40aaa-3108-4fb2-9bbb-60498b7423cd

---

# **Dueling Network Architectures for Deep Reinforcement Learning**

- **作者**：Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas，均来自Google DeepMind, London, UK

## **摘要**
- 近年来，深度表征在强化学习中取得了许多成功。尽管许多应用采用了传统架构（如卷积网络、LSTMs或自编码器），本文提出了一种新的神经网络架构，用于无模型强化学习。
- **双重网络**：代表两个独立的估计器，一个用于状态价值函数，另一个用于状态依赖的动作优势函数。这种分解的主要好处是在不改变底层强化学习算法的情况下，跨动作泛化学习。
- **优势**：在存在许多相似价值动作的情况下，这种架构能够带来更好的策略评估。此外，双重架构使得我们的RL代理能够在Atari 2600领域超越最先进的水平。

## **1. 引言**
- 过去几年，深度学习在机器学习的可扩展性和性能上做出了巨大的贡献。其中一个激动人心的应用是强化学习（RL）和控制的顺序决策设置。
- **现状**：尽管取得了显著进展，但大多数RL方法仍然使用标准的神经网络，如卷积网络、MLPs、LSTMs和自编码器。这些进步主要集中在设计改进的控制和RL算法上，或者仅仅是将现有的神经网络架构整合到RL方法中。
- **本文贡献**：提出了一种新的网络架构——双重架构，显式分离状态值和（状态依赖的）动作优势的表示。这种架构有两个流程，分别代表价值和优势函数，同时共享一个公共的输入，提出的网络架构旨在更适合无模型的RL，可以轻松地与现有和未来的RL算法结合使用。

---

接下来，我将继续总结第二页的内容。

第二页的内容主要继续介绍了双重网络架构的细节及其相关工作。下面是对这一页内容的摘要：

## **双重网络架构细节**

- 双重网络架构由两个流程组成，这两个流程分别代表价值函数和优势函数，通过一个特殊的聚合层结合起来，以产生状态-动作价值函数\(Q\)的估计，如图1所示。这种架构被视为单个\(Q\)网络的替代，它有两个流程，用于替换现有算法（如DQN）中的流行单流\(Q\)网络。
- 双重网络能够自动产生状态价值函数和优势函数的独立估计，无需任何额外的监督。
- **直观理解**：双重架构能够学习哪些状态是有价值的（或无价值的），而无需学习每个状态下每个动作的效果。这在动作对环境没有任何相关影响的状态中特别有用。
- **示例**：图2显示了针对输入视频计算的训练完成的价值和优势流程的雅可比矩阵生成的显著性图。这些显著性图展示了两个不同时间步骤的价值和优势显著性图。在一个时间步骤中，价值网络流程关注于道路和地平线（新车出现的位置），以及得分。而优势流程在前方没有车时几乎不关注视觉输入，因为此时的动作选择几乎无关紧要。然而，在第二个时间步骤中，优势流程因为前方紧急有车，所以其动作选择变得非常相关。

## **1.1 相关工作**

- 维护独立的价值和优势函数的概念可以追溯到Baird（1993）。在Baird最初的优势更新算法中，共享的贝尔曼残差更新方程被分解为两个更新：一个用于状态价值函数，另一个用于其相关的优势函数。优势更新被证明在简单连续时间领域中比Q学习收敛更快。
- 双重架构代表了单个深度模型中的价值\(V(s)\)和优势\(A(s; a)\)函数，其输出结合这两者以产生状态-动作价值\(Q(s; a)\)。与优势更新不同，这种表示和算法通过构建解耦。
- 在政策梯度中有关优势函数的长期历史，始于Sutton等人（2000）。最近的一项工作中，Schulman等人（2015）在线估计优势值以降低政策梯度算法的方差。
- 已有多次尝试使用深度强化学习玩Atari游戏，包括多个研究团队的工作。

---

接下来，将总结第三页的内容。

第三页的内容主要介绍了强化学习的背景知识，包括基础概念和DQN的关键组成部分。以下是这一页内容的摘要：

## **2. 背景**

- 讨论了顺序决策设置中的基本框架，其中一个代理(agent)在离散时间步与环境\(E\)交互。例如，在Atari领域，代理感知到由\(M\)个图像帧组成的视频\(s_t = (x_{t-M+1}; \dots; x_t) \in S\)。
- 代理选择一个来自离散集合的动作\(a_t \in A = \{1, \dots, |A|\}\)，并观察由游戏仿真器产生的奖励信号\(r_t\)。
- 代理寻求最大化预期折现回报，定义折现回报为\(R_t = \sum_{\tau=t}^\infty \gamma^{\tau-t} r_\tau\)，其中\(\gamma \in [0, 1]\)是折现因子。
- 对于遵循随机策略\(\beta\)的代理，状态-动作对\((s, a)\)和状态\(s\)的价值定义为 \(Q_\beta(s, a) = E[R_t | s_t = s, a_t = a, \beta]\) 和 \(V_\beta(s) = E_{a \sim \beta(s)}[Q_\beta(s, a)]\)。

### **Q函数和价值函数**
- 前述的状态-动作价值函数（简称Q函数）可以通过动态规划递归地计算。定义最优\(Q^*(s, a) = \max_\beta Q_\beta(s, a)\)。
- 在确定性策略下，\(a = \arg \max_{a'} Q^*(s, a')\)，由此可得\(V^*(s) = \max_a Q^*(s, a)\)。
- 最优Q函数满足贝尔曼方程：\(Q^*(s, a) = E_{s'}[r + \max_{a'} Q^*(s', a') | s, a]\)。

### **优势函数**
- 定义另一个重要量——优势函数\(A_\beta(s, a) = Q_\beta(s, a) - V_\beta(s)\)，直观上，\(V\)函数衡量处于特定状态\(s\)的好处，而\(Q\)函数则衡量选择特定动作的价值。

### **DQN关键组成部分**
- **经验回放**：代理累积来自多个剧集的经验数据集\(D_t = \{e_1, e_2, \dots, e_t\}\)，其中\(e_t = (s_t, a_t, r_t, s_{t+1})\)。通过从\(D\)中均匀随机采样小批量经验来训练\(Q\)网络，经验回放通过重复使用经验样本进行多次更新来提高数据效率，并且通过从回放缓冲区中均匀采样减少样本间的相关性，从而减少方差。
- **双重深度Q网络**：上一节描述了DQN的主要组件。这部分内容介绍了DQN的关键思想，包括经验回放和目标网络的使用，以及如何通过梯度下降更新网络参数来改进算法的稳定性。

---

接下来，将总结第四页的内容。

第四页内容进一步介绍了双重DQN（DDQN）学习算法、优先回放和双重网络架构的核心设计理念。以下是对这一页内容的摘要：

## **2.2 双重深度Q网络（DDQN）**
- 在Q学习和DQN中，`max`操作符用于同时选择和评估一个动作，这可能导致过于乐观的价值估计。DDQN通过使用改进的目标来缓解这个问题，其核心在于使用一个网络来选择动作，而使用另一个独立的网络来评估该动作的价值。

## **2.3 优先回放**
- 基于DDQN之上的优先经验回放进一步改善了状态艺术水平。其关键思想是增加那些有高预期学习进步（通过绝对TD误差的代理测量）的经验元组的回放概率，这导致了比均匀经验回放更快的学习和在大多数Atari游戏中更好的最终策略质量。

## **3. 双重网络架构**
- **核心洞察**：对于许多状态，不必估计每个动作选择的价值。例如，在Enduro游戏中，仅在碰撞即将发生时，知道向左还是向右移动才重要。在某些状态下，知道采取哪个动作至关重要，而在许多其他状态下，动作的选择对发生的事情没有影响。
- **架构设计**：双重网络由两个全连接层流组成，一个输出标量值\(V(s; \theta; \beta)\)，表示状态价值；另一个输出维度为|A|的向量\(A(s, a; \theta; \alpha)\)，表示优势函数。这里\(\theta\)表示卷积层的参数，\(\alpha\)和\(\beta\)分别是两个全连接层流的参数。
- 通过优势函数的定义，我们可能会被诱导构造如下的聚合模块：\(Q(s, a; \theta; \alpha; \beta) = V(s; \theta; \beta) + A(s, a; \theta; \alpha)\)。然而，这种表示在实践中表现不佳，因为直接使用这个等式会导致识别性问题。
- **解决识别性问题**：为了解决优势函数估计器的识别性问题，可以强制在选择的动作上优势函数估计器为零。即，网络的最后一个模块实现如下映射：\(Q(s, a; \theta; \alpha; \beta) = V(s; \theta; \beta) + (A(s, a; \theta; \alpha) - \max_{a'} A(s, a'; \theta; \alpha))\)。

---

接下来，将总结第五页的内容。

第五页深入讨论了双重网络架构的实现细节和在简化环境中的实验结果。以下是对这一页内容的摘要：

## **双重网络架构实现细节**

- 引入了一个替代模块，用平均值替换了`max`运算符：\(Q(s, a; \theta; \alpha; \beta) = V(s; \theta; \beta) + (A(s, a; \theta; \alpha) - \frac{1}{|A|}\sum_{a'}A(s, a'; \theta; \alpha))\)。这种方法虽然使得\(V\)和\(A\)的原始语义有所偏离，但增加了优化的稳定性。
- 减去平均值有助于解决识别性问题，但不会改变\(A\)（因而是\(Q\)）值的相对排名，保留了基于\(Q\)值的任何贪婪或\(\epsilon\)-贪婪策略。
- 重要的是，等式(9)被视为网络的一部分，而不是作为一个单独的算法步骤，并且通过反向传播就可以进行训练，无需任何额外的监督或算法修改。

## **简化环境中的实验结果**

- 设计了一个名为“走廊”的简化环境，以比较单流\(Q\)网络架构和双重网络架构的性能。在这个环境中，代理从环境的左下角开始，必须移动到右上角以获得最大奖励。
- 实验使用了\(\epsilon\)-贪婪策略作为行为策略，并比较了在具有5、10和20个动作的三种走廊环境变体上的单流\(Q\)架构和双重架构的性能。
- 性能通过与真实状态值的平方误差(SE)进行衡量。实验结果表明，当动作数量增加时，双重架构比传统的\(Q\)网络表现更好。在双重网络中，\(V(s; \theta; \beta)\)学习了一个跨多个类似动作共享的通用值，从而导致更快的收敛。

---

接下来，将总结第六页的内容。
廊”环境中的实验设置和结果，以及在广泛的Atari游戏上评估双重网络架构的性能。以下是对这一页内容的摘要：

## **走廊环境实验结果**
- **图3**展示了“走廊”环境的示意图以及在具有5、10和20个动作的三种变体中策略评估的平方误差。结果显示，双重网络（Duel）一致地优于传统的单流网络（Single），随着动作数量的增加，性能差距增大。这表明，在动作空间较大的控制任务中，双重网络通常能够比传统的单流网络更快收敛。

## **Atari游戏的一般性评估**
- 对提出的方法在57款Atari游戏组成的街机学习环境（ALE）上进行了全面评估。挑战在于部署单一算法和架构，使用固定的超参数集，仅依靠原始像素观察和游戏奖励来学习所有游戏。这是一个非常苛刻的环境，因为它包含了大量高度多样化的游戏，且观察结果是高维的。
- 网络架构在低级卷积结构上与DQN相同，但在全连接层中采用了双重网络架构，其中价值流（Value stream）有一个输出，优势流（Advantage stream）有与有效动作数量相等的输出。使用等式(9)描述的模块组合价值和优势流。

## **实验设置和超参数**
- 采用了与van Hasselt等人（2015）相同的优化器和超参数，除了将学习率稍微降低。为了增加稳定性，通过`1/sqrt(2)`重新缩放了传入最后一个卷积层的组合梯度，并将梯度剪裁到不超过10的范数。这些设置不是深度强化学习中的标准做法，但在循环网络训练中较为常见。

## **评估和比较**
- 为了单独评估双重架构的贡献，使用与上述完全相同的程序重新训练了DDQN的单流网络，并将其称为Single Clip，而van Hasselt等人（2015）原始训练的模型称为Single。
- 游戏开始时，为了提供随机起始位置，最多执行30个无操作（no-op）动作。评估方法是测量在人类和基线代理分数中更好者上的分数改进百分比。

---

接下来，将总结第七页的内容。

第七页展示了双重架构相比于基线单流网络在Atari游戏全套57款游戏上的改进，并讨论了使用人类起始点作为更健壮的性能衡量方法。以下是对这一页内容的摘要：

## **双重架构性能提升**
- 图4展示了双重架构相比于van Hasselt等人（2015年）的单流网络的性能提升，使用等式(10)描述的度量方法。条形图向右表示双重网络如何超越单流网络的性能。
- 结果表明，双重网络（Duel Clip）在相似容量的Single Clip网络上有显著的改进，并且相比于van Hasselt等人（2015年）的基线（Single）也有相当大的提升。

## **Atari游戏全套评估结果**
- 表1总结了57款Atari游戏的平均和中位数得分，以人类表现的百分比衡量。双重架构在“30 no-ops”和“Human Starts”两种启动条件下都优于单流变体。尤其是在“Human Starts”条件下，双重架构比单流基线在70.2%（40/57）的游戏上表现更好，在具有18个动作的游戏中，双重架构的表现优于83.3%（25/30）。

## **与优先经验回放的结合**
- 双重架构可以轻松与其他算法改进结合使用。尤其是，经验回放的优先级排序已被证明可以显著提高Atari游戏的性能。由于优先排序和双重架构针对学习过程的不同方面，它们的结合十分有前景。最终实验调查了双重架构与优先经验回放结合使用的效果，使用DDQN的优先变体（Prior. Single）作为新的基线算法。

---

接下来，将总结第八页的内容。

第八页继续讨论了双重网络架构与优先经验回放结合使用的结果，并引入了显著性图以更好地理解双重网络架构的作用。以下是对这一页内容的摘要：

## **与优先经验回放的结合结果**
- 图5展示了双重架构相比于优先DDQN基线的性能提升，使用与图4相同的度量方法。双重架构在大多数游戏上相比单流基线取得了显著改善。
- 通过优先经验回放和双重网络结合，相对于之前的最佳结果，在流行的ALE基准测试中取得了巨大的进步。使用30次无操作动作进行游戏初始化时，观察到的平均和中位数分数分别为591%和172%。

## **显著性图的使用**
- 为了更好地理解双重网络架构的作用，研究中利用了显著性图来可视化网络在做决策时关注的区域。图2展示了在Enduro游戏中两个不同时间步的价值和优势显著性图。价值流关注可能影响未来表现的地平线区域和得分，而优势流更关注即将发生碰撞的车辆。

## **双重架构的讨论**
- 双重架构的优势部分在于其有效学习状态价值函数的能力。在双重架构中，每次更新\(Q\)值时，价值流\(V\)也会被更新，这与单流架构中仅更新一个动作的值不同，其他所有动作的值保持不变。这种更频繁的价值流更新在我们的方法中分配了更多资源给\(V\)，从而允许更好地近似状态值，这对于基于时间差异的方法（如Q学习）的有效性至关重要。
- 对于给定状态，\(Q\)值之间的差异通常与\(Q\)的幅度相比非常小。例如，在Seaquest游戏中，训练完成后的平均动作差距（最佳动作和次佳动作的\(Q\)值差）大约为0.04，而这些状态的平均状态值大约为15。这种规模上的差异可能导致更新中的微小噪声引起动作的重新排序，从而使得近似贪婪策略突然切换。双重架构通过更频繁地更新价值流，有助于减少这种现象。

---

接下来，将总结第九页的内容。

第九页总结了研究的主要结论，并提供了参考文献列表的一部分。以下是对这一页内容的摘要：

## **6. 结论**
- 介绍了一种新的神经网络架构，该架构在深度Q网络中解耦了价值和优势，同时共享一个公共的特征学习模块。结合一些算法改进，这种新的双重架构在挑战性的Atari领域相比现有方法取得了显著的改进，提出的结果是这一流行领域的新最佳水平。

## **参考文献**
- 列出了研究中引用的部分文献，包括多个与深度学习、强化学习、视觉注意力、经验回放优先级、策略梯度方法等相关的关键研究和技术报告。这些参考文献为读者提供了深入了解研究背景和相关工作的途径。

## **显著性图的讨论**
- 虽然第九页没有详细讨论显著性图，但在前文中提到，显著性图被用来可视化决策过程中网络关注的区域，有助于理解双重网络架构是如何作用的。

## **总结**
- 研究通过引入双重架构来改进深度强化学习，实现了在Atari游戏领域的显著性能提升。此外，结合优先经验回放进一步提高了双重架构的性能。研究的主要结论是，双重架构在挑战性的Atari领域相比现有方法取得了显著的改进，提出的结果是这一流行领域的新最佳水平。