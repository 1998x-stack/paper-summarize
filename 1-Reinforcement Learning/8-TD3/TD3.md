https://chat.openai.com/share/a54dbcff-259f-4934-9c1e-c71fa4a5bf1a

---


### **Addressing Function Approximation Error in Actor-Critic Methods**

- **摘要**: 本文主要讨论了在基于值的强化学习方法（如深度Q学习）中，函数逼近误差导致的值估计过高和策略次优的问题，并展示了这一问题在actor-critic设置中依然存在。我们提出了一种新的机制来最小化这一问题对actor和critic的影响。我们的算法基于双Q学习，通过取一对critics之间的最小值来限制过估计。我们探讨了目标网络与过估计偏差之间的联系，并建议延迟策略更新以减少每次更新的误差并进一步提高性能。我们在OpenAI gym任务套件上评估了我们的方法，在每个测试的环境中都超过了现有技术的水平。

- **1. 引言**: 在离散动作空间的强化学习问题中，函数逼近误差导致的值过估计问题已被广泛研究。然而，在连续控制领域中使用actor-critic方法时，类似的问题却鲜少被提及。本文展示了过估计偏差和时间差分方法中误差累积在actor-critic设置中的存在，并提出了解决这些问题的方法，大大超越了当前技术的水平。过估计偏差是Q学习中的一个特性，即噪声值估计的最大化会导致一致的过估计。在函数逼近的环境下，由于估计器的不准确性，这种噪声不可避免。时间差分学习的本质进一步夸大了这种不准确性，其中一个状态的值函数估计使用后续状态的估计进行更新。我们提出的方法通过使用一对独立训练的critics，在actor-critic格式中适应双Q学习，减少了偏差估计的问题。我们还引入了一种新的正则化策略，通过使用类SARSA更新来引导相似动作估计，进一步减少方差。

**主要观点**:
- 函数逼近误差在actor-critic方法中存在，并可能导致值过估计和策略次优。
- 通过双Q学习和使用一对critics的最小值来限制过估计。
- 目标网络和策略更新的延迟有助于减少误差和提高性能。
- 在OpenAI gym任务上，提出的方法超过了现有技术的水平。

接下来，我将继续提取并总结下一页的内容。

### **Addressing Function Approximation Error in Actor-Critic Methods**

#### **2. 相关工作**
- 研究了函数逼近误差及其对强化学习算法中偏差和方差的影响。本工作集中讨论由估计误差引起的两个结果：过估计偏差和高方差积累。
- 存在多种方法减少由函数逼近和策略优化在Q学习中引起的过估计偏差。双Q学习使用两个独立估计器来做出无偏值估计。其他方法直接专注于减少方差，最小化对早期高方差估计的过度拟合，或通过纠正项。
- 已考虑值估计的方差直接用于风险规避和探索，但未与过估计偏差联系起来。
- 通过最小化每个时间步的误差大小或混合离策略和蒙特卡洛回报来处理时间差分学习中由误差积累引起的方差问题。

#### **3. 背景**
- 强化学习考虑的是一个智能体与其环境互动的范式，目标是学习最大化奖励的行为。在每个离散时间步骤，给定状态`s`，智能体根据其策略选择动作`a`，接收奖励`r`和环境的新状态`s'`。
- 强化学习的目标是找到最优策略`π*`，通过参数`θ`，最大化期望回报`J(θ) = E[R₀]`。在连续控制中，参数化策略可以通过取期望回报的梯度`∂θJ(θ)`来更新。在actor-critic方法中，策略（即actor）可以通过确定性策略梯度算法更新。
- Q函数`Qπ(s, a) = E[Rt | s, a]`，即在状态`s`执行动作`a`并之后遵循策略π的期望回报，被称为critic或值函数。
- 在Q学习中，值函数可以使用时间差分学习（基于Bellman方程）进行学习。对于大状态空间，值可以用可微的函数逼近器`Qθ(s, a)`，在深度Q学习中，网络通过使用次级冻结的目标网络`Qθ'(s, a)`进行时间差分学习来更新，以保持固定目标。

**主要观点**:
- 研究函数逼近误差对偏差和方差的影响，以及减少过估计偏差和方差的方法。
- 强化学习的基本概念，包括智能体、环境、状态、动作、奖励、回报、策略、值函数和时间差分学习。

接下来，我将继续提取并总结下一页的内容。

### **Addressing Function Approximation Error in Actor-Critic Methods**

#### **4. 过估计偏差**

- 在具有离散动作的Q学习中，值估计使用贪婪目标更新，但如果目标受到误差影响，则最大值估计加上其误差通常会大于真实最大值，从而导致即使是最初均值为零的误差也会导致值更新结果出现一致的过估计偏差，进而通过Bellman方程传播。这是问题所在，因为由函数逼近引起的错误是不可避免的。
- 在离散动作设置中，过估计偏差是分析最大化的一个明显产物，而在actor-critic设置中，通过梯度下降更新策略时，过估计偏差的存在和影响不那么明显。我们首先证明，在一些基本假设下，确定性策略梯度中的值估计将是一个过估计，然后提出一个削减双Q学习的变体，在actor-critic设置中减少过估计偏差。

##### **4.1. Actor-Critic中的过估计偏差**
- 在actor-critic方法中，策略根据近似critic的值估计进行更新。本节假设策略使用确定性策略梯度进行更新，并显示更新引发过估计偏差。通过比较DDPG算法在OpenAI gym环境Hopper-v1和Walker2d-v1上学习时的值估计随时间变化，我们发现实际上的值估计与真实值估计之间存在明显的过估计现象。

**主要观点**:
- 过估计偏差在离散动作Q学习和actor-critic设置中均存在，且可能导致值更新结果的一致过估计。
- 提出在actor-critic框架中使用削减双Q学习的方法，以减少过估计偏差，通过实证分析证明了其在实际应用中的有效性。

接下来，我将继续提取并总结下一页的内容。

### **Addressing Function Approximation Error in Actor-Critic Methods**

#### **4.2. 削减双Q学习用于Actor-Critic**
- 提出了一种新的削减双Q学习（Clipped Double Q-learning）变体，可以替代任何actor-critic方法中的critic。在双Q学习中，通过维护两个独立的值估计来解耦贪婪更新和值函数，每个值估计用于更新另一个。如果这些值估计是独立的，它们可以用来对使用相对值估计选定的动作进行无偏估计。
- 在实践中，由于actor-critic中策略变化缓慢，当前和目标网络太相似，无法进行独立估计，提供的改进有限。相反，可以使用原始的双Q学习公式，配对演员（两个actors）和critics，提出简单地将较少偏见的值估计上限为偏见较大的估计，从而在两个估计之间取最小值，以给出我们的削减双Q学习算法的目标更新。
- 削减双Q学习的目标更新不会引入任何额外的过估计，相比过估计偏差，这种更新规则可能引入低估偏差，但这更可取，因为低估的动作值不会通过策略更新明确传播。
- 在实现中，可以通过使用单个actor针对`Q_1`进行优化来降低计算成本。如果`Q_2 > Q_1`，则更新与标准更新相同，不会引入额外偏差。如果`Q_2 < Q_1`，这表明发生了过估计，值将类似于双Q学习被降低。

#### **5. 地址方差**
- 虽然第4节处理了方差对过估计偏差的贡献，我们还主张应直接解决方差本身。通过将函数逼近误差视为随机变量，我们可以看到最小化操作符应该为具有较低方差估计误差的状态提供更高的值，因为一组随机变量的期望最小值随着随机变量的方差增加而减少。这意味着等式中的最小化将导致对低方差值估计的状态的偏好，从而导致更安全的策略更新和稳定的学习目标。

**主要观点**:
- 新提出的削减双Q学习方法旨在减少actor-critic框架中的过估计偏差，通过在两个独立估计之间取最小值来实现。
- 该方法可能引入低估偏差，但相比过估计，这种偏差在策略更新中的传播将被明显减少。
- 直接解决方差问题，通过最小化操作降低方差，以实现更安全的策略更新和稳定的学习目标。

接下来，我将继续提取并总结下一页的内容。

### **Addressing Function Approximation Error in Actor-Critic Methods**

#### **5.1. 累积误差**
- 由于时间差分更新，估计的值函数是基于后续状态的估计构建的，因此会累积误差。尽管可以期望单个更新的误差较小，但这些估计误差可以累积，导致可能出现大的过估计偏差和次优策略更新。在函数逼近设置中，这一问题尤为严重，因为Bellman方程从未被精确满足，每次更新都会留下一些残留的TD误差。
- 值估计实际上是预期回报减去预期折扣未来TD误差的近似值。如果值估计是未来奖励和估计误差的函数，则其估计的方差将与未来奖励和估计误差的方差成正比。给定较大的折扣因子，如果每次更新的误差未得到控制，方差可以随着每次更新迅速增长。

#### **5.2. 目标网络和延迟更新**
- 高方差估计为策略更新提供了噪声梯度，已知这会降低学习速度并实际上损害性能。本节强调在每次更新中最小化误差的重要性，建立目标网络和估计误差之间的联系，并提出修改actor-critic学习程序以减少方差的方法。
- 更新策略时不使用目标网络会增加波动性，但所有更新率在考虑固定策略时都会导致类似的收敛行为。然而，当策略与当前值估计一起训练时，使用快速更新的目标网络会导致高度发散的行为。
- 结果表明，没有目标网络时发生的分歧是由于使用高方差值估计进行策略更新造成的。为了在多次更新中减少误差，并且在高误差状态下的策略更新会导致发散行为，策略网络应该比值网络更新的频率低，首先要最小化误差，然后再引入策略更新。我们建议延迟策略更新，直到值误差尽可能小。修改是在对critic进行了固定次数的更新后，再更新策略和目标网络。为了确保TD误差保持小，我们更新策略和目标网络的频率要低于值网络，通过这种方式可以先减少误差，然后再进行策略更新，以期达到更稳定和准确的学习效果。

**主要观点**:
- 高方差的估计会对策略更新产生噪声梯度，降低学习速度并损害性能。
- 时间差分更新导致的误差累积可能会导致大的过估计偏差和次优策略更新。
- 通过最小化每次更新的误差，并利用目标网络和延迟更新策略减少方差，可以提高学习效率和策略性能。
- 建议使用目标网络和延迟更新策略来应对由于误差累积和高方差导致的问题，通过降低更新频率，先减少误差再进行策略更新。

接下来，我将继续提取并总结下一页的内容。

### **Addressing Function Approximation Error in Actor-Critic Methods**

#### **5.3. 目标策略平滑正则化**
- 针对确定性策略可能对值估计中的狭窄峰值过拟合的问题，我们引入了一种正则化策略：目标策略平滑。这种方法模仿了SARSA的学习更新，强调相似动作应具有相似的值。通过修改训练过程显式强制相似动作之间的关系，我们提出通过在目标动作周围的小范围内拟合值来平滑值估计，实践中通过向目标策略添加少量随机噪声并在小批量上平均来近似这个期望。

#### **算法1: TD3 (Twin Delayed Deep Deterministic policy gradient)**
- 我们提出了Twin Delayed Deep Deterministic policy gradient算法（TD3），在Deep Deterministic Policy Gradient算法（DDPG）的基础上，通过应用第4.2节、第5.2节和第5.3节描述的修改来增加在函数逼近误差考虑下的稳定性和性能。TD3保持一对critics和一个单独的actor。每个时间步骤，我们将一对critics更新为由目标策略选定的动作的最小目标值。
- 每隔`d`次迭代，根据确定性策略梯度算法更新策略，针对`Q_1`。TD3的总结在算法1中给出。

**主要观点**:
- 目标策略平滑正则化旨在通过正则化减少因函数逼近误差引起的不准确性，提高学习的稳定性。
- TD3算法通过维持一对critics和单一actor，并在目标策略选定的动作上采取最小目标值策略，解决过估计偏差和函数逼近误差的问题。
- 通过引入目标策略平滑正则化和对策略及目标网络的延迟更新，TD3算法提高了在考虑函数逼近误差时的稳定性和性能。

接下来，我将继续提取并总结下一页的内容。

### **Addressing Function Approximation Error in Actor-Critic Methods**

#### **6.1 评估**
- 为了评估我们的算法，我们在MuJoCo连续控制任务套件上测量其性能，通过OpenAI Gym接口（图4）。为了可重复比较，我们使用Brockman等人（2016）的原始任务集，没有对环境或奖励进行修改。
- 在我们对DDPG的实现中，使用了一个具有400和300隐藏节点的两层前馈神经网络，对于actor和critic都使用了ReLU激活函数，actor的输出后接一个tanh单元。与原始DDPG不同，critic的第一层同时接收状态和动作作为输入。使用Adam优化器更新网络参数，学习率为1e-3。每个时间步后，使用从包含代理整个历史的回放缓存中均匀采样的100个转换的小批量训练网络。
- 目标策略平滑通过向目标actor网络选择的动作添加高斯噪声实现，噪声限制在(-0.5, 0.5)内，延迟策略更新包括每隔`d`次迭代只更新actor和目标critic网络，其中`d=2`。目标网络的更新率为0.005。
- 在稳定长度环境（HalfCheetah-v1和Ant-v1）的前10000时间步和其他环境的前1000时间步，使用纯探索策略以消除策略初始参数的依赖性。之后，使用离策略探索策略，在每个动作上添加高斯噪声N(0, 0.1)。
- 每个任务运行100万时间步，每5000时间步进行评估，评估报告平均返回超过10次试验的半个标准差的阴影区域。曲线为了视觉清晰度而统一平滑处理。

#### **实验结果概览**
- 学习曲线显示了OpenAI gym连续控制任务的性能。表1显示了在100万时间步的10次试验中的最大平均回报。TD3在每个测试环境中都表现出色，与DDPG、PPO、TRPO、ACKTR和SAC等其他算法相比，TD3在许多任务中达到了最高的平均回报。
- 这些结果证实了TD3算法在处理函数逼近误差时，通过使用一对critics和延迟更新策略以及目标策略平滑来增强稳定性和性能的有效性。
